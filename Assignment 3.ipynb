{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92778525",
   "metadata": {},
   "source": [
    "# Assignment 3: Non-Linear Models and Validation Metrics (37 total marks)\n",
    "### Due: October 24 at 11:59pm\n",
    "\n",
    "### Name: Aemen Mohsin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce31b39a",
   "metadata": {},
   "source": [
    "### In this assignment, you will need to write code that uses non-linear models to perform classification and regression tasks. You will also be asked to describe the process by which you came up with the code. More details can be found below. Please cite any websites or AI tools that you used to help you with this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf275ca7",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b67a661",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee2d2c3",
   "metadata": {},
   "source": [
    "## Part 1: Regression (14.5 marks)\n",
    "\n",
    "For this section, we will be continuing with the concrete example from yellowbrick. You will need to compare these results to the results from the previous assignment. Please use the results from the solution if you were unable to complete Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8219f163",
   "metadata": {},
   "source": [
    "### Step 1: Data Input (0.5 marks)\n",
    "\n",
    "The data used for this task can be downloaded using the yellowbrick library: \n",
    "https://www.scikit-yb.org/en/latest/api/datasets/concrete.html\n",
    "\n",
    "Use the yellowbrick function `load_concrete()` to load the concrete dataset into the feature matrix `X` and target vector `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2af8bd32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TO DO: Import concrete dataset from yellowbrick library\n",
    "\n",
    "from yellowbrick.datasets import load_concrete\n",
    "X, y = load_concrete()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fea4cc",
   "metadata": {},
   "source": [
    "### Step 2: Data Processing (0 marks)\n",
    "\n",
    "Data processing was completed in the previous assignment. No need to repeat here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a245d00",
   "metadata": {},
   "source": [
    "### Step 3: Implement Machine Learning Model\n",
    "\n",
    "1. Import the Decision Tree, Random Forest and Gradient Boosting Machines regression models from sklearn\n",
    "2. Instantiate the three models with `max_depth = 5`. Are there any other parameters that you will need to set?\n",
    "3. Implement each machine learning model with `X` and `y`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f994e31",
   "metadata": {},
   "source": [
    "### Step 4: Validate Model\n",
    "\n",
    "Calculate the average training and validation accuracy using mean squared error with cross-validation. To do this, you will need to set `scoring='neg_mean_squared_error'` in your `cross_validate` function and negate the results (multiply by -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc3f7a8",
   "metadata": {},
   "source": [
    "### Step 5: Visualize Results (4 marks)\n",
    "\n",
    "1. Create a pandas DataFrame `results` with columns: Training accuracy and Validation accuracy, and index: DT, RF and GB\n",
    "2. Add the accuracy results to the `results` DataFrame\n",
    "3. Print `results`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fdc93a78",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Training Accuracy  Validation Accuracy\n",
      "DT          47.822974            74.045335\n",
      "RF          30.296363            47.614708\n",
      "GB           3.694308            23.546500\n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE FOR STEPS 3-5\n",
    "# Note: for any random state parameters, you can use random_state = 0\n",
    "# HINT: USING A LOOP TO STORE THE DATA IN YOUR RESULTS DATAFRAME WILL BE MORE EFFICIENT\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import make_scorer, mean_squared_error\n",
    "\n",
    "# Creating a training and testing split (train - 80%, test - 20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=0)\n",
    "\n",
    "# Instantiate and fit models with parameters: max_depth = 5, random_state = 0\n",
    "\n",
    "# Decision Tree\n",
    "tree = DecisionTreeRegressor(max_depth=5, random_state=0)\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "# Creating validation model\n",
    "scores_tree = cross_validate(tree, X_train, y_train, cv=5, \n",
    "                        scoring='neg_mean_squared_error',\n",
    "                       return_train_score=True)\n",
    "\n",
    "# Creating data for training and validation\n",
    "for label_pair in [ ('train_score', 'train_score')]:\n",
    "    train1 = (-scores_tree[label_pair[0]].mean())\n",
    "for label_pair in [ ('test_score', 'validation_score')]:\n",
    "    val1 = (-scores_tree[label_pair[0]].mean())\n",
    "\n",
    "# Random Forest\n",
    "forest = RandomForestRegressor(max_depth=5, random_state=0)\n",
    "forest.fit(X_train, y_train)\n",
    "\n",
    "# Creating validation model\n",
    "scores_forest = cross_validate(forest, X_train, y_train, cv=5, \n",
    "                        scoring='neg_mean_squared_error',\n",
    "                       return_train_score=True)\n",
    "\n",
    "# Creating data for training and validation\n",
    "for label_pair in [ ('train_score', 'train_score')]:\n",
    "    train2 = (-scores_forest[label_pair[0]].mean())\n",
    "for label_pair in [ ('test_score', 'validation_score')]:\n",
    "    val2 = (-scores_forest[label_pair[0]].mean())\n",
    "\n",
    "# Gradient Boosting\n",
    "gbrt = GradientBoostingRegressor(max_depth=5, random_state=0)\n",
    "gbrt.fit(X_train, y_train)\n",
    "\n",
    "# Creating validation model\n",
    "scores_gbrt = cross_validate(gbrt, X_train, y_train, cv=5, \n",
    "                        scoring='neg_mean_squared_error',\n",
    "                       return_train_score=True)\n",
    "\n",
    "# Creating data for training and validation\n",
    "for label_pair in [ ('train_score', 'train_score')]:\n",
    "    train3 = (-scores_gbrt[label_pair[0]].mean())\n",
    "for label_pair in [ ('test_score', 'validation_score')]:\n",
    "    val3 = (-scores_gbrt[label_pair[0]].mean())\n",
    "    \n",
    "# Creating the Dataframe\n",
    "data = {'Training Accuracy': [train1, train2, train3],\n",
    "            'Validation Accuracy': [val1, val2, val3]}\n",
    "results = pd.DataFrame(data, index=['DT', 'RF', 'GB'])\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31715a9d",
   "metadata": {},
   "source": [
    "Repeat the step above to print the R2 score instead of the mean-squared error. For this case, you can use `scoring='r2'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83539f47",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Training Accuracy  Validation Accuracy\n",
      "DT           0.830437             0.735184\n",
      "RF           0.892634             0.830004\n",
      "GB           0.986903             0.916155\n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "\n",
    "# Decision Tree\n",
    "# Creating validation model\n",
    "scores_tree = cross_validate(tree, X_train, y_train, cv=5, \n",
    "                        scoring='r2',\n",
    "                       return_train_score=True)\n",
    "\n",
    "# Creating data for training and validation\n",
    "for label_pair in [ ('train_score', 'train_score')]:\n",
    "    train1 = (scores_tree[label_pair[0]].mean())\n",
    "for label_pair in [ ('test_score', 'validation_score')]:\n",
    "    val1 = (scores_tree[label_pair[0]].mean())\n",
    "\n",
    "# Random Forest\n",
    "# Creating validation model\n",
    "scores_forest = cross_validate(forest, X_train, y_train, cv=5, \n",
    "                        scoring='r2',\n",
    "                       return_train_score=True)\n",
    "\n",
    "# Creating data for training and validation\n",
    "for label_pair in [ ('train_score', 'train_score')]:\n",
    "    train2 = (scores_forest[label_pair[0]].mean())\n",
    "for label_pair in [ ('test_score', 'validation_score')]:\n",
    "    val2 = (scores_forest[label_pair[0]].mean())\n",
    "\n",
    "# Gradient Boosting\n",
    "# Creating validation model\n",
    "scores_gbrt = cross_validate(gbrt, X_train, y_train, cv=5, \n",
    "                        scoring='r2',\n",
    "                       return_train_score=True)\n",
    "\n",
    "# Creating data for training and validation\n",
    "for label_pair in [ ('train_score', 'train_score')]:\n",
    "    train3 = (scores_gbrt[label_pair[0]].mean())\n",
    "for label_pair in [ ('test_score', 'validation_score')]:\n",
    "    val3 = (scores_gbrt[label_pair[0]].mean())\n",
    "    \n",
    "# Creating the Dataframe\n",
    "data = {'Training Accuracy': [train1, train2, train3],\n",
    "            'Validation Accuracy': [val1, val2, val3]}\n",
    "results = pd.DataFrame(data, index=['DT', 'RF', 'GB'])\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5257a98",
   "metadata": {},
   "source": [
    "### Questions (6 marks)\n",
    "1. How do these results compare to the results using a linear model in the previous assignment? Use values.\n",
    "1. Out of the models you tested, which model would you select for this dataset and why?\n",
    "1. If you wanted to increase the accuracy of the tree-based models, what would you do? Provide two suggestions.\n",
    "\n",
    "#### Answers\n",
    "\n",
    "1. When comparing these results to the linear model, they yeilded much better results. The mean squared error should be close to 0 and this time it was much lower compared to the linear models. When using linear regression, the model yielded a score of 110.345501 for training and 95.635335 for validation. When using the decision tree, training was 47.822974 and validation was 74.045335. For random forest, the training score was 30.296363 and validation was 47.614708. Lastly, for gradient boosting, the training score was 3.694308 and validation score was 23.546500. The errors were lower for all three non-linear models, however, they could havae been overfitted since the mse for training was much lower than validation. When looking at R2 scores, for linear regression, the training score was 0.609071 and validation was 0.636898. When using the decision tree, training was 0.830437 and validation was 0.735184. For random forest, the training score was 0.982634 and validation was 0.830004. Lastly, for gradient boosting, the training score was 0.986903 and validation score was 0.916155. These scores should be close to 1, so they were much improved compared to the linear model. These R2 scores were much closer in training and accuracry compared to mean squared error.\n",
    "\n",
    "2. Out of the models tested, I would select gradient boosting, since the mse was the closest to 0 and the R2 score was very close to 1. I might try to reduce the depth for this model since the difference between the training and validation score does indicate some overfitting.\n",
    "\n",
    "3. My first suggestion would be to pre-prune the tree or decrease the depth of the models since they seem to be a bit overfitted. My second suggestion would be to post-prune the tree or remove collapsing nodes that don't have much information in them. Both of these would prevent overfitting the tree.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b238f4",
   "metadata": {},
   "source": [
    "### Process Description (4 marks)\n",
    "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
    "1. Where did you source your code?\n",
    "1. In what order did you complete the steps?\n",
    "1. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
    "1. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93097bfe",
   "metadata": {},
   "source": [
    "*DESCRIBE YOUR PROCESS HERE*\n",
    "\n",
    "1. The source code was derived mainly from the lecture examples: Decision Trees, Decision Trees Examples and Ensembles. The only additional resource was to get the cross-validation portion correct for mse which was a bit unclear. For this, stack exchange was used: https://stackoverflow.com/questions/48244219/is-sklearn-metrics-mean-squared-error-the-larger-the-better-negated\n",
    "2. The steps were completed in the order they were asked.\n",
    "3. No generative AI was necessary since the lecture code and internet was simple enough to easily modify.\n",
    "4. The only challenge faced was using cross-validation with mean squared error since I was unsure how to actually accomplish that using lecture material. For this I had to consult stack exchange."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c6de86",
   "metadata": {},
   "source": [
    "## Part 2: Classification (17.5 marks)\n",
    "\n",
    "You have been asked to develop code that can help the user classify different wine samples. Following the machine learning workflow described in class, write the relevant code in each of the steps below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9d33a8",
   "metadata": {},
   "source": [
    "### Step 1: Data Input (2 marks)\n",
    "\n",
    "The data used for this task can be downloaded from UCI: https://archive.ics.uci.edu/dataset/109/wine\n",
    "\n",
    "Use the pandas library to load the dataset. You must define the column headers if they are not included in the dataset \n",
    "\n",
    "You will need to split the dataset into feature matrix `X` and target vector `y`. Which column represents the target vector?\n",
    "\n",
    "Print the size and type of `X` and `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "33583c67",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(178, 13)\n",
      "(178,)\n",
      "Alcohol                         float64\n",
      "Malic Acid                      float64\n",
      "Ash                             float64\n",
      "Alcalinity of Ash               float64\n",
      "Magnesium                         int64\n",
      "Total phenols                   float64\n",
      "Flavanoids                      float64\n",
      "Nonflavanoid Phenols            float64\n",
      "Proanthocyanins                 float64\n",
      "Color Intensity                 float64\n",
      "Hue                             float64\n",
      "OD280/OD315 of Diluted Wines    float64\n",
      "Proline                           int64\n",
      "dtype: object\n",
      "int64\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Import wine dataset\n",
    "\n",
    "def load_wine():\n",
    "    '''Load and pre-process wine data\n",
    "    \n",
    "    it will be downloaded from\n",
    "    https://archive.ics.uci.edu/dataset/109/wine/wine.data\n",
    "    \n",
    "    return: data(DataFrame)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    import os\n",
    "    import requests\n",
    "    \n",
    "    \n",
    "    file_url = 'https://archive.ics.uci.edu/dataset/109/wine/wine.data'\n",
    "    file_name = file_url.split('/')[-1]\n",
    "    \n",
    "    if not os.path.isfile(file_name):\n",
    "        print('Downloading from {}'.format(file_url))\n",
    "        r = requests.get(file_url)\n",
    "        with open(file_name,'wb') as output_file:\n",
    "            output_file.write(r.content)\n",
    "        \n",
    "    data = pd.read_csv(file_name, sep=',', names= ['Class', 'Alcohol', 'Malic Acid', 'Ash', 'Alcalinity of Ash', \n",
    "                'Magnesium', 'Total phenols', 'Flavanoids', 'Nonflavanoid Phenols', \n",
    "                'Proanthocyanins', 'Color Intensity', 'Hue', 'OD280/OD315 of Diluted Wines',\n",
    "                'Proline'])\n",
    "    return data\n",
    "\n",
    "# Method to load data if file in directory:\n",
    "#data = pd.read_csv('wine.data', sep=',')\n",
    "#df = pd.DataFrame(data)\n",
    "#df.columns = ['Class', 'Alcohol', 'Malic Acid', 'Ash', 'Alcalinity of Ash', \n",
    "                # 'Magnesium', 'Total phenols', 'Flavanoids', 'Nonflavanoid Phenols', \n",
    "                # 'Proanthocyanins', 'Color Intensity', 'Hue', 'OD280/OD315 of Diluted Wines',\n",
    "                # 'Proline']\n",
    "            \n",
    "data = load_wine()\n",
    "#print(data.info())\n",
    "\n",
    "# Creating feature matrix and target vector (class) to determine each of the 3 wines\n",
    "X = data.drop(columns='Class')\n",
    "y = data['Class']\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(X.dtypes)\n",
    "print(y.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156db208",
   "metadata": {},
   "source": [
    "### Step 2: Data Processing (1.5 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28af110",
   "metadata": {},
   "source": [
    "Print the first five rows of the dataset to inspect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ea266921",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Malic Acid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Alcalinity of Ash</th>\n",
       "      <th>Magnesium</th>\n",
       "      <th>Total phenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>Nonflavanoid Phenols</th>\n",
       "      <th>Proanthocyanins</th>\n",
       "      <th>Color Intensity</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD280/OD315 of Diluted Wines</th>\n",
       "      <th>Proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Class  Alcohol  Malic Acid   Ash  Alcalinity of Ash  Magnesium  \\\n",
       "0      1    14.23        1.71  2.43               15.6        127   \n",
       "1      1    13.20        1.78  2.14               11.2        100   \n",
       "2      1    13.16        2.36  2.67               18.6        101   \n",
       "3      1    14.37        1.95  2.50               16.8        113   \n",
       "4      1    13.24        2.59  2.87               21.0        118   \n",
       "\n",
       "   Total phenols  Flavanoids  Nonflavanoid Phenols  Proanthocyanins  \\\n",
       "0           2.80        3.06                  0.28             2.29   \n",
       "1           2.65        2.76                  0.26             1.28   \n",
       "2           2.80        3.24                  0.30             2.81   \n",
       "3           3.85        3.49                  0.24             2.18   \n",
       "4           2.80        2.69                  0.39             1.82   \n",
       "\n",
       "   Color Intensity   Hue  OD280/OD315 of Diluted Wines  Proline  \n",
       "0             5.64  1.04                          3.92     1065  \n",
       "1             4.38  1.05                          3.40     1050  \n",
       "2             5.68  1.03                          3.17     1185  \n",
       "3             7.80  0.86                          3.45     1480  \n",
       "4             4.32  1.04                          2.93      735  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834fc8fe",
   "metadata": {},
   "source": [
    "Check to see if there are any missing values in the dataset. If necessary, select an appropriate method to fill-in the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "97c6e9dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Class                           0\n",
       "Alcohol                         0\n",
       "Malic Acid                      0\n",
       "Ash                             0\n",
       "Alcalinity of Ash               0\n",
       "Magnesium                       0\n",
       "Total phenols                   0\n",
       "Flavanoids                      0\n",
       "Nonflavanoid Phenols            0\n",
       "Proanthocyanins                 0\n",
       "Color Intensity                 0\n",
       "Hue                             0\n",
       "OD280/OD315 of Diluted Wines    0\n",
       "Proline                         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070956af",
   "metadata": {},
   "source": [
    "How many samples do we have of each type of wine?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b37a6fd9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 1:  59\n",
      "Class 2:  71\n",
      "Class 3:  48\n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "print(\"Class 1: \", len(data[data.Class == 1]))\n",
    "print(\"Class 2: \", len(data[data.Class == 2]))\n",
    "print(\"Class 3: \", len(data[data.Class == 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e6c46f",
   "metadata": {},
   "source": [
    "### Step 3: Implement Machine Learning Model\n",
    "\n",
    "1. Import `SVC` and `DecisionTreeClassifier` from sklearn\n",
    "2. Instantiate models as `SVC()` and `DecisionTreeClassifier(max_depth = 3)`\n",
    "3. Implement the machine learning model with `X` and `y`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0870b0d2",
   "metadata": {},
   "source": [
    "### Step 4: Validate Model \n",
    "\n",
    "Calculate the average training and validation accuracy using `cross_validate` for the two different models listed in Step 3. For this case, use `scoring='accuracy'`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0bbd83",
   "metadata": {},
   "source": [
    "### Step 5: Visualize Results (4 marks)\n",
    "\n",
    "#### Step 5.1: Compare Models\n",
    "1. Create a pandas DataFrame `results` with columns: Training accuracy and Validation accuracy\n",
    "2. Add the data size, training and validation accuracy for each dataset to the `results` DataFrame\n",
    "3. Print `results`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "be4b5c0a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Data Size  Training Accuracy  Validation Accuracy\n",
      "0  (178, 13)           0.698882             0.662808\n",
      "1  (178, 13)           0.994721             0.929310\n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE FOR STEPS 3-5\n",
    "# Note: for any random state parameters, you can use random_state = 0\n",
    "# HINT: USING A LOOP TO STORE THE DATA IN YOUR RESULTS DATAFRAME WILL BE MORE EFFICIENT\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Creating a training and testing split (train - 80%, test - 20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=0)\n",
    "\n",
    "# Instantiating models\n",
    "svc = SVC()\n",
    "tree = DecisionTreeClassifier(max_depth=3, random_state=0)\n",
    "\n",
    "# Fitting models\n",
    "svc.fit(X_train, y_train)\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "# Creating validation model\n",
    "# SVC\n",
    "scores_svc = cross_validate(svc, X_train, y_train, cv=5, \n",
    "                        scoring='accuracy',\n",
    "                       return_train_score=True)\n",
    "\n",
    "# Creating data for training and validation\n",
    "for label_pair in [ ('train_score', 'train_score')]:\n",
    "    train1 = (scores_svc[label_pair[0]].mean())\n",
    "for label_pair in [ ('test_score', 'validation_score')]:\n",
    "    val1 = (scores_svc[label_pair[0]].mean())\n",
    "\n",
    "# tree\n",
    "scores_tree = cross_validate(tree, X_train, y_train, cv=5, \n",
    "                        scoring='accuracy',\n",
    "                       return_train_score=True)\n",
    "\n",
    "# Creating data for training and validation\n",
    "for label_pair in [ ('train_score', 'train_score')]:\n",
    "    train2 = (scores_tree[label_pair[0]].mean())\n",
    "for label_pair in [ ('test_score', 'validation_score')]:\n",
    "    val2 = (scores_tree[label_pair[0]].mean())\n",
    "\n",
    "# Creating the Dataframe\n",
    "data = {'Data Size': [X.shape, X.shape],\n",
    "            'Training Accuracy': [train1, train2],\n",
    "            'Validation Accuracy': [val1, val2]}\n",
    "results = pd.DataFrame(data)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e17878",
   "metadata": {},
   "source": [
    "#### Step 5.2: Visualize Classification Errors\n",
    "Which method gave the highest accuracy? Use this method to print the confusion matrix and classification report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "44b091a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(max_depth=3, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(max_depth=3, random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier(max_depth=3, random_state=0)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TO DO: Implement best model\n",
    "# The best model was the Decision Tree Classifier\n",
    "\n",
    "tree.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "09d21b59",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(170.97222222222223, 0.5, 'true value')"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x550 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TO DO: Print confusion matrix using a heatmap\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "mat = confusion_matrix(y_test, tree.predict(X_test))\n",
    "\n",
    "sns.heatmap(mat, square=True, annot=True, cbar=False)\n",
    "plt.xlabel('predicted value')\n",
    "plt.ylabel('true value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "5ef95947",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 1       0.93      1.00      0.97        14\n",
      "     class 2       1.00      0.94      0.97        16\n",
      "     class 3       1.00      1.00      1.00         6\n",
      "\n",
      "    accuracy                           0.97        36\n",
      "   macro avg       0.98      0.98      0.98        36\n",
      "weighted avg       0.97      0.97      0.97        36\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Print classification report\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "target_names = ['class 1', 'class 2', 'class 3']\n",
    "print(classification_report(y_test, tree.predict(X_test), target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf319621",
   "metadata": {},
   "source": [
    "### Questions (6 marks)\n",
    "1. How do the training and validation accuracy change depending on the method used? Explain with values.\n",
    "1. What are two reasons why the support vector machines model did not work as well as the tree-based model?\n",
    "1. How many samples were incorrectly classified in step 5.2? \n",
    "1. In this case, is maximizing precision or recall more important? Why?\n",
    "\n",
    "*YOUR ANSWERS HERE*\n",
    "\n",
    "1. The training and validation accuracy for the decision SVC classifier was 0.698882 for training and 0.662808 for validation, while for Decision Tree was a training score of 0.994721 and validation score of 0.929310. This means the Decision tree by far was the best model for this dataset.\n",
    "2. Support vector machines are more complex models compared to decidion trees which means they don't always perform better. For support vector machines to work properly, the type of kernel needs to be appropriate. It is also better for low-dimensional and high-dimensional data. In our case, the data was in between and the parameters (kernel, regularization) were not focused on so the model was not properly formulated for the data. Lastly, the initial definition of SVC that we have used only supports 2 classes. These factors are not important for decision trees, so that model worked a lot better.\n",
    "3. Only 1 sample was incorrectly classified in the confusion matrix.\n",
    "4. In this case, precision is more important than recall, since we are interested in classifying the wine correctly. We do not need to focus on minimizing false negatives for recall, since it is not important; rather, we want to minimize false positives so identification of the wine is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664ff8ae",
   "metadata": {},
   "source": [
    "### Process Description (4 marks)\n",
    "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
    "1. Where did you source your code?\n",
    "1. In what order did you complete the steps?\n",
    "1. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
    "1. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e837da",
   "metadata": {},
   "source": [
    "*DESCRIBE YOUR PROCESS HERE*\n",
    "\n",
    "1. The source code was derived mainly from the lecture examples: Decision Trees, Decision Trees Examples and SVM. The only additional resource was to get the classification report, since that was not something we had done before. For this, scikit-learn documentation was used: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html\n",
    "2. The steps were completed in the order they were asked.\n",
    "3. No generative AI was necessary since the lecture code and internet was simple enough to easily modify.\n",
    "4. The only challenge faced was loading the wine data from the website. Prior to this, all datasets were extracted from scikit-learn but this time required some trial and error to upload the data properly with correct formatting and creating the target vector and feature matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd7358d",
   "metadata": {},
   "source": [
    "## Part 3: Observations/Interpretation (3 marks)\n",
    "\n",
    "Describe any pattern you see in the results. Relate your findings to what we discussed during lectures. Include data to justify your findings.\n",
    "\n",
    "\n",
    "*ADD YOUR FINDINGS HERE*\n",
    "\n",
    "For the decision tree, the training and validation accuracy was higher than for SVM. I though this to be a bit strange at first, since I thought SVM would perform better because it uses less features to create a decision boundary, according to the discussion. I though 13 features were little, but after performing the classification, I see that even this many features can be too much for the model and make it too computationally intensive. This time, for the decision tree, the model seemed to work very well and produced high scores. However, the difference between the training and testing set does indicate some overfitting of the model, causing high variance. The same can be true for the concrete example. The scores were much better than last lab but there might have been some overfitting and the models would benefit from regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd97b6ac",
   "metadata": {},
   "source": [
    "## Part 4: Reflection (2 marks)\n",
    "Include a sentence or two about:\n",
    "- what you liked or disliked,\n",
    "- found interesting, confusing, challangeing, motivating\n",
    "while working on this assignment.\n",
    "\n",
    "\n",
    "*ADD YOUR THOUGHTS HERE*\n",
    "\n",
    "I enjoyed working with the same dataset as last lab and trying more models on it to see which one was the best for that sample and what the reason might have been. I had some difficulty with the visualization part in Part 2, but in the end was able to decipher both the confusion matrix and the report. I had not read a 3x3 matrix before so this was a new aspect for me to learn in this lab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa21e53b",
   "metadata": {},
   "source": [
    "## Part 5: Bonus Question (3 marks)\n",
    "\n",
    "Repeat Part 2 and compare the support vector machines model used to `LinearSVC(max_iter=5000)`. Does using `LinearSVC` improve the results? Why or why not?\n",
    "\n",
    "Is `LinearSVC` a good fit for this dataset? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "30fea72e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mohsi\\miniconda3\\envs\\ensf-ml\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mohsi\\miniconda3\\envs\\ensf-ml\\lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mohsi\\miniconda3\\envs\\ensf-ml\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mohsi\\miniconda3\\envs\\ensf-ml\\lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mohsi\\miniconda3\\envs\\ensf-ml\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mohsi\\miniconda3\\envs\\ensf-ml\\lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mohsi\\miniconda3\\envs\\ensf-ml\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mohsi\\miniconda3\\envs\\ensf-ml\\lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mohsi\\miniconda3\\envs\\ensf-ml\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mohsi\\miniconda3\\envs\\ensf-ml\\lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mohsi\\miniconda3\\envs\\ensf-ml\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mohsi\\miniconda3\\envs\\ensf-ml\\lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data Size</th>\n",
       "      <th>Training Accuracy</th>\n",
       "      <th>Validation Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(178, 13)</td>\n",
       "      <td>0.91728</td>\n",
       "      <td>0.894581</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Data Size  Training Accuracy  Validation Accuracy\n",
       "0  (178, 13)            0.91728             0.894581"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Creating a training and testing split (train - 80%, test - 20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=0)\n",
    "\n",
    "# Instantiating models\n",
    "svc = LinearSVC(max_iter=5000)\n",
    "\n",
    "# Fitting models\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "# Creating validation model\n",
    "# SVC\n",
    "scores_svc = cross_validate(svc, X_train, y_train, cv=5, \n",
    "                        scoring='accuracy',\n",
    "                       return_train_score=True)\n",
    "\n",
    "# Creating data for training and validation\n",
    "for label_pair in [ ('train_score', 'train_score')]:\n",
    "    train1 = (scores_svc[label_pair[0]].mean())\n",
    "for label_pair in [ ('test_score', 'validation_score')]:\n",
    "    val1 = (scores_svc[label_pair[0]].mean())\n",
    "\n",
    "# Creating the Dataframe\n",
    "data = {'Data Size': [X.shape],\n",
    "            'Training Accuracy': [train1],\n",
    "            'Validation Accuracy': [val1]}\n",
    "pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabc68a4",
   "metadata": {},
   "source": [
    "*ANSWER HERE*\n",
    "\n",
    "Yes, using LinearSVC did improve the scores by a lot. The testing score is now 0.91728 and validation score is 0.894581. This might be because the initial definition of SVC, as we have done, does not support multi-class classificaitons. LinearSVC, on the other hand, does support this, which might be why it yielded a much higher training and accuracy score. All three classes could be classified here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241c3b12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
